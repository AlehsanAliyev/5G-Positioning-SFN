{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b939fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned Scanner data shape: (22390, 19)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load raw Scanner data\n",
    "df = pd.read_excel(\"../data/raw/5G_Scanner.xlsx\", sheet_name=\"Series Formatted Data\")\n",
    "\n",
    "# --- 1. Drop completely useless or empty columns ---\n",
    "drop_cols = [\n",
    "    'Unnamed: 12', 'Unnamed: 20', 'Unnamed: 28',  # 100% missing\n",
    "    'Message', 'Time'  # non-predictive\n",
    "]\n",
    "df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors=\"ignore\")\n",
    "\n",
    "# --- 2. Drop very sparse features (>95% missing) ---\n",
    "too_sparse_cols = [\n",
    "    'NR_Scan_SSB_RSRQ_SortedBy_RSRP_6',\n",
    "    'NR_Scan_SSB_RSRP_SortedBy_RSRP_6',\n",
    "    'NR_Scan_PCI_SortedBy_RSRP_6',\n",
    "    'NR_Scan_SSB_SINR_SortedBy_RSRP_6',\n",
    "    'NR_Scan_SSB_RSRQ_SortedBy_RSRP_5',\n",
    "    'NR_Scan_SSB_SINR_SortedBy_RSRP_5',\n",
    "    'NR_Scan_SSB_RSRP_SortedBy_RSRP_5',\n",
    "    'NR_Scan_PCI_SortedBy_RSRP_5',\n",
    "    'NR_Scan_SSB_RSRQ_SortedBy_RSRP_4',\n",
    "    'NR_Scan_SSB_SINR_SortedBy_RSRP_4',\n",
    "    'NR_Scan_PCI_SortedBy_RSRP_4',\n",
    "    'NR_Scan_SSB_RSRP_SortedBy_RSRP_4'\n",
    "]\n",
    "df = df.drop(columns=[col for col in too_sparse_cols if col in df.columns], errors=\"ignore\")\n",
    "\n",
    "# --- 3. Impute missing values for valid columns ---\n",
    "\n",
    "# Fill PCI columns with -1 (non-detect)\n",
    "pci_cols = [col for col in df.columns if \"PCI\" in col]\n",
    "df[pci_cols] = df[pci_cols].fillna(-1)\n",
    "\n",
    "# Fill RSRP/RSRQ/SINR with domain-informed defaults\n",
    "rsrp_cols = [col for col in df.columns if \"RSRP\" in col]\n",
    "rsrq_cols = [col for col in df.columns if \"RSRQ\" in col]\n",
    "sinr_cols = [col for col in df.columns if \"SINR\" in col]\n",
    "\n",
    "df[rsrp_cols] = df[rsrp_cols].fillna(-200)  # dBm\n",
    "df[rsrq_cols] = df[rsrq_cols].fillna(-30)   # dB\n",
    "df[sinr_cols] = df[sinr_cols].fillna(-10)   # dB\n",
    "\n",
    "# Fill NR_ARFCN (frequency) with median\n",
    "if \"NR_Scan_NR_ARFCN\" in df.columns:\n",
    "    df[\"NR_Scan_NR_ARFCN\"] = df[\"NR_Scan_NR_ARFCN\"].fillna(df[\"NR_Scan_NR_ARFCN\"].median())\n",
    "\n",
    "# --- 4. Drop object-type columns if any remain ---\n",
    "df = df.drop(columns=df.select_dtypes(include='object').columns)\n",
    "\n",
    "# --- 5. Drop rows with missing target (lat/lon) ---\n",
    "df = df.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
    "\n",
    "print(\"‚úÖ Cleaned Scanner data shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ba3d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No missing values remain. Data is clean.\n"
     ]
    }
   ],
   "source": [
    "# Show summary of missing values\n",
    "missing = df.isna().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "if not missing.empty:\n",
    "    print(\"\\nüîç Columns with Missing Values:\\n\")\n",
    "    print(missing.to_frame(\"Missing Count\").assign(Missing_Percent=lambda x: 100 * x[\"Missing Count\"] / len(df)))\n",
    "else:\n",
    "    print(\"‚úÖ No missing values remain. Data is clean.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c5e065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç Scanner Feature Importances (Latitude):\n",
      "NR_Scan_PCI_SortedBy_RSRP_0         0.348242\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_0    0.277253\n",
      "NR_Scan_PCI_SortedBy_RSRP_1         0.159040\n",
      "NR_Scan_NR_ARFCN                    0.059013\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_2    0.035488\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_2    0.032136\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_0    0.020707\n",
      "NR_Scan_PCI_SortedBy_RSRP_2         0.016793\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_0    0.012527\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_1    0.009139\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_1    0.007996\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_1    0.006572\n",
      "NR_Scan_PCI_SortedBy_RSRP_3         0.006163\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_3    0.002719\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_2    0.002238\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_3    0.002123\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_3    0.001852\n",
      "dtype: float64\n",
      "\n",
      "üß≠ Scanner Feature Importances (Longitude):\n",
      "NR_Scan_PCI_SortedBy_RSRP_0         0.268340\n",
      "NR_Scan_PCI_SortedBy_RSRP_1         0.247916\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_2    0.131296\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_1    0.111653\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_0    0.066747\n",
      "NR_Scan_NR_ARFCN                    0.038944\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_0    0.031165\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_1    0.024209\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_0    0.023703\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_1    0.021495\n",
      "NR_Scan_PCI_SortedBy_RSRP_2         0.017841\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_3    0.009692\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_2    0.002605\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_2    0.001972\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_3    0.001038\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_3    0.000931\n",
      "NR_Scan_PCI_SortedBy_RSRP_3         0.000453\n",
      "dtype: float64\n",
      "\n",
      "üî¢ PCA Explained Variance Ratios:\n",
      "[0.57630249 0.15843985 0.07697845 0.06052278 0.03864881 0.02891177\n",
      " 0.01914351 0.01496576 0.01001581 0.008304  ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# ---- 1. Split features and target ----\n",
    "X = df.drop(columns=[\"Latitude\", \"Longitude\"], errors=\"ignore\")\n",
    "y_lat = df[\"Latitude\"]\n",
    "y_lon = df[\"Longitude\"]\n",
    "\n",
    "# ---- 2. Train Random Forests ----\n",
    "rf_lat = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_lat.fit(X, y_lat)\n",
    "importances_lat = pd.Series(rf_lat.feature_importances_, index=X.columns)\n",
    "\n",
    "rf_lon = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_lon.fit(X, y_lon)\n",
    "importances_lon = pd.Series(rf_lon.feature_importances_, index=X.columns)\n",
    "\n",
    "# ---- 3. Print Feature Importances ----\n",
    "print(\"\\nüìç Scanner Feature Importances (Latitude):\")\n",
    "print(importances_lat.sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nüß≠ Scanner Feature Importances (Longitude):\")\n",
    "print(importances_lon.sort_values(ascending=False))\n",
    "\n",
    "# ---- 4. PCA Analysis ----\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "print(\"\\nüî¢ PCA Explained Variance Ratios:\")\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d30639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Cumulative Contribution:\n",
      " NR_Scan_PCI_SortedBy_RSRP_0         0.308291\n",
      "NR_Scan_PCI_SortedBy_RSRP_1         0.511769\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_0    0.683769\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_2    0.750536\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_1    0.810360\n",
      "NR_Scan_NR_ARFCN                    0.859339\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_0    0.881544\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_0    0.903389\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_2    0.922119\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_2    0.939490\n",
      "NR_Scan_PCI_SortedBy_RSRP_2         0.956807\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_1    0.973482\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_1    0.987515\n",
      "NR_Scan_SSB_RSRP_SortedBy_RSRP_3    0.993422\n",
      "NR_Scan_PCI_SortedBy_RSRP_3         0.996730\n",
      "NR_Scan_SSB_SINR_SortedBy_RSRP_3    0.998608\n",
      "NR_Scan_SSB_RSRQ_SortedBy_RSRP_3    1.000000\n",
      "dtype: float64\n",
      "\n",
      "‚úÖ Selected 13 important features.\n"
     ]
    }
   ],
   "source": [
    "# --- Average feature importances ---\n",
    "avg_importance = (importances_lat + importances_lon) / 2\n",
    "avg_importance = avg_importance.sort_values(ascending=False)\n",
    "\n",
    "# --- Cumulative contribution ---\n",
    "cumulative = avg_importance.cumsum() / avg_importance.sum()\n",
    "print(\"\\nüìà Cumulative Contribution:\\n\", cumulative)\n",
    "\n",
    "# --- Select features covering ~99% contribution ---\n",
    "selected_features = cumulative[cumulative < 0.99].index.tolist()\n",
    "print(f\"\\n‚úÖ Selected {len(selected_features)} important features.\")\n",
    "\n",
    "# --- Final selected feature matrix ---\n",
    "X_selected = X[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c177e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = pd.read_excel(\"../data/raw/5G_Scanner.xlsx\", sheet_name=\"Series Formatted Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f3d87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan2 = pd.read_excel(\"../data/Sample_Data_2/5G_Scanner.xlsx\", sheet_name=\"Series Formatted Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d78c8d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columns in X_selected that are covered by both dl2 and dl_data:\n",
      " {'NR_Scan_SSB_RSRP_SortedBy_RSRP_2', 'NR_Scan_PCI_SortedBy_RSRP_1', 'NR_Scan_SSB_RSRP_SortedBy_RSRP_1', 'NR_Scan_NR_ARFCN', 'NR_Scan_SSB_RSRQ_SortedBy_RSRP_1', 'NR_Scan_SSB_RSRQ_SortedBy_RSRP_2', 'NR_Scan_PCI_SortedBy_RSRP_0', 'NR_Scan_SSB_SINR_SortedBy_RSRP_0', 'NR_Scan_SSB_SINR_SortedBy_RSRP_1', 'NR_Scan_SSB_RSRP_SortedBy_RSRP_0', 'NR_Scan_SSB_RSRQ_SortedBy_RSRP_0', 'NR_Scan_SSB_SINR_SortedBy_RSRP_2', 'NR_Scan_PCI_SortedBy_RSRP_2'}\n",
      "\n",
      "‚ùå Columns in X_selected that are NOT in both dl2 and dl_data:\n",
      " set()\n",
      "\n",
      "‚ÑπÔ∏è Extra common columns NOT used in X_selected (might be useful features):\n",
      " {'NR_Scan_SSB_SINR_SortedBy_RSRP_4', 'NR_Scan_SSB_SINR_SortedBy_RSRP_6', 'Time', 'NR_Scan_PCI_SortedBy_RSRP_6', 'NR_Scan_SSB_RSRP_SortedBy_RSRP_4', 'NR_Scan_SSB_SINR_SortedBy_RSRP_5', 'NR_Scan_SSB_RSRP_SortedBy_RSRP_6', 'NR_Scan_SSB_RSRP_SortedBy_RSRP_3', 'NR_Scan_PCI_SortedBy_RSRP_4', 'NR_Scan_SSB_RSRQ_SortedBy_RSRP_6', 'NR_Scan_PCI_SortedBy_RSRP_5', 'NR_Scan_SSB_RSRQ_SortedBy_RSRP_4', 'Latitude', 'NR_Scan_SSB_RSRQ_SortedBy_RSRP_5', 'NR_Scan_SSB_RSRP_SortedBy_RSRP_5', 'Message', 'Longitude', 'NR_Scan_SSB_SINR_SortedBy_RSRP_3', 'NR_Scan_SSB_RSRQ_SortedBy_RSRP_3', 'NR_Scan_PCI_SortedBy_RSRP_3'}\n"
     ]
    }
   ],
   "source": [
    "# Get sets of column names\n",
    "columns_dl2 = set(scan2.columns)\n",
    "columns_dl_data = set(scan.columns)\n",
    "columns_X_selected = set(X_selected.columns)\n",
    "\n",
    "# Find common columns between dl2 and dl_data\n",
    "common_columns = columns_dl2 & columns_dl_data\n",
    "\n",
    "# Compare X_selected columns to common columns\n",
    "covered_columns = columns_X_selected & common_columns\n",
    "missing_in_common = columns_X_selected - common_columns\n",
    "extra_in_common = common_columns - columns_X_selected\n",
    "\n",
    "# Print results\n",
    "print(\"‚úÖ Columns in X_selected that are covered by both dl2 and dl_data:\\n\", covered_columns)\n",
    "print(\"\\n‚ùå Columns in X_selected that are NOT in both dl2 and dl_data:\\n\", missing_in_common)\n",
    "print(\"\\n‚ÑπÔ∏è Extra common columns NOT used in X_selected (might be useful features):\\n\", extra_in_common)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ed1449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NR_Scan_PCI_SortedBy_RSRP_0', 'NR_Scan_PCI_SortedBy_RSRP_1',\n",
       "       'NR_Scan_SSB_RSRP_SortedBy_RSRP_0', 'NR_Scan_SSB_RSRP_SortedBy_RSRP_2',\n",
       "       'NR_Scan_SSB_RSRP_SortedBy_RSRP_1', 'NR_Scan_NR_ARFCN',\n",
       "       'NR_Scan_SSB_RSRQ_SortedBy_RSRP_0', 'NR_Scan_SSB_SINR_SortedBy_RSRP_0',\n",
       "       'NR_Scan_SSB_RSRQ_SortedBy_RSRP_2', 'NR_Scan_SSB_SINR_SortedBy_RSRP_2',\n",
       "       'NR_Scan_PCI_SortedBy_RSRP_2', 'NR_Scan_SSB_SINR_SortedBy_RSRP_1',\n",
       "       'NR_Scan_SSB_RSRQ_SortedBy_RSRP_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee958204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1261.3229\n",
      "Epoch 10, Train Loss: 1223.7283\n",
      "Epoch 20, Train Loss: 1163.9115\n",
      "Epoch 30, Train Loss: 1060.7528\n",
      "Epoch 40, Train Loss: 899.8802\n",
      "Epoch 50, Train Loss: 680.5199\n",
      "Epoch 60, Train Loss: 440.2289\n",
      "Epoch 70, Train Loss: 245.7580\n",
      "Epoch 80, Train Loss: 117.0056\n",
      "Epoch 90, Train Loss: 54.4688\n",
      "‚úÖ Model saved as scanner_model.pth\n",
      "\n",
      "üìç Scanner Model Test MSE: 35.1528\n",
      "üìè Mean Localization Error: 6.29 meters\n",
      "üìè Median Localization Error: 4.23 meters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# ---- 1. Data Preparation ----\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "y = df[[\"Latitude\", \"Longitude\"]].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# ---- 2. Model Definition ----\n",
    "class StructuredMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_uncertainty=False):\n",
    "        super().__init__()\n",
    "        self.output_uncertainty = output_uncertainty\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.coord_head = nn.Linear(64, 2)\n",
    "        self.uncertainty_head = nn.Linear(64, 1) if output_uncertainty else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        coords = self.coord_head(x)\n",
    "        log_var = self.uncertainty_head(x) if self.output_uncertainty else torch.zeros(len(x), 1)\n",
    "        return coords, log_var\n",
    "\n",
    "# ---- 3. Training ----\n",
    "model = StructuredMLP(input_dim=X_train.shape[1], output_uncertainty=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds, _ = model(X_train_tensor)\n",
    "    loss = criterion(preds, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"scanner_model.pth\")\n",
    "print(\"‚úÖ Model saved as scanner_model.pth\")\n",
    "# ---- 4. Evaluation ----\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test, _ = model(X_test_tensor)\n",
    "    mse = mean_squared_error(y_test_tensor.numpy(), preds_test.numpy())\n",
    "    print(f\"\\nüìç Scanner Model Test MSE: {mse:.4f}\")\n",
    "\n",
    "    # Euclidean localization error\n",
    "    error_dist = np.linalg.norm(preds_test.numpy() - y_test_tensor.numpy(), axis=1)\n",
    "    print(f\"üìè Mean Localization Error: {error_dist.mean():.2f} meters\")\n",
    "    print(f\"üìè Median Localization Error: {np.median(error_dist):.2f} meters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0becb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input dimensions (e.g., 15 for uplink, 16 for scanner, etc.)\n",
    "input_dim = X_test_tensor.shape[1]\n",
    "\n",
    "# Initialize model and load weights\n",
    "model = StructuredMLP(input_dim=input_dim, output_uncertainty=True)\n",
    "model.load_state_dict(torch.load(\"scanner_model.pth\"))  # <- Change filename if needed\n",
    "model.eval()\n",
    "\n",
    "# Run predictions\n",
    "with torch.no_grad():\n",
    "    preds, log_var = model(X_test_tensor)\n",
    "    predictions = preds.numpy()\n",
    "    uncertainty = torch.exp(log_var).numpy() if model.output_uncertainty else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34601b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downlink\n",
    "model_dl = StructuredMLP(input_dim=X_dl_test_tensor.shape[1])\n",
    "model_dl.load_state_dict(torch.load(\"downlink_model.pth\"))\n",
    "model_dl.eval()\n",
    "\n",
    "# Uplink\n",
    "model_ul = StructuredMLP(input_dim=X_ul_test_tensor.shape[1])\n",
    "model_ul.load_state_dict(torch.load(\"uplink_model.pth\"))\n",
    "model_ul.eval()\n",
    "\n",
    "# Scanner\n",
    "model_sc = StructuredMLP(input_dim=X_sc_test_tensor.shape[1])\n",
    "model_sc.load_state_dict(torch.load(\"scanner_model.pth\"))\n",
    "model_sc.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# For Latitude\n",
    "r2_lat = r2_score(y_test[:, 0], y_pred[:, 0])\n",
    "\n",
    "# For Longitude\n",
    "r2_lon = r2_score(y_test[:, 1], y_pred[:, 1])\n",
    "\n",
    "print(f\"R¬≤ Score - Latitude: {r2_lat:.4f}\")\n",
    "print(f\"R¬≤ Score - Longitude: {r2_lon:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
